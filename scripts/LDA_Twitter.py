# -*- coding: utf-8 -*-
"""LDA_Twitter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e0FbpRoHwsvsdqw3hLXgYMMG6jcrbN6v
"""

import pandas as pd
import io

df = pd.read_csv('/home/renato/Documentos/tweets.csv')


import numpy as np
import altair as alt
from sklearn.feature_extraction.text import CountVectorizer
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

import re
from time import time


"""Pre processamento"""

stop_words = set(stopwords.words("portuguese"))
stop_words.update(['que', 'até', 'esse', 
                    'essa', 'pro', 'pra',
                    'oi', 'lá', 'blá', 'bb', 
                    'bbm', 'abm', 'cbm', 
                    'dbm', 'dos', 
                    'ltda', 'editora']), 

clean_tweets = []
for w in range(len(df.TWEET)):
  tweet = df['TWEET'].iloc[w]

  # remove special characters and digits
  tweet  = re.sub("(\\d|\\W)+|\w*\d\w*"," ",tweet )
  tweet = ' '.join(s for s in tweet.split() if (not any(c.isdigit() for c in s)) and len(s) > 2)
  clean_tweets.append(tweet)


"""LDA funciona baseado em frequências de palavras, então usaremos TFs, e não TF-IDFs."""

# COUNT vectorizer
tf_vectorizer = CountVectorizer(
        min_df = 30,
        max_df = 0.5,
        max_features = 10000,
        stop_words = stop_words, 
        ngram_range = (1,2)
  )

#transform
vec_text = tf_vectorizer.fit_transform(clean_tweets)

#returns a list of words.
words = tf_vectorizer.get_feature_names()

print(vec_text.shape)
print(len(words))



"""Encontrando tópicos
O resultado terá

uma matriz que descreve a relação entre palavras e tópicos
uma matriz que descreve a relação entre documentos e tópicos
Existe uma outra implementação de LDA popular em python chamada Gensim.
"""

from sklearn.decomposition import LatentDirichletAllocation

def print_top_words(model, feature_names, n_top_words):
  for topic_idx, topic in enumerate(model.components_):
    print("\n--\nTopic #{}: ".format(topic_idx + 1))
    message = ", ".join([feature_names[i]
                          for i in topic.argsort()[:-n_top_words - 1:-1]])
    print(message)
  print()

def display_topics(W, H, feature_names, documents, no_top_words, no_top_documents):
    for topic_idx, topic in enumerate(H):
        print("\n--\nTopic #{}: ".format(topic_idx + 1))
        print(", ".join([feature_names[i]
                for i in topic.argsort()[:-no_top_words - 1:-1]]).upper())
        top_d_idx = np.argsort(W[:,topic_idx])[::-1][0:no_top_documents]
        for d in top_d_idx: 
          doc_data = df[['USUARIO', 'TWEET']].iloc[d]
          print('{} - {} : \t{:.2f}'.format(doc_data[0], doc_data[1], W[d, topic_idx]))

lda = LatentDirichletAllocation(n_components=15, 
                                learning_method='online', # 'online' equivale a minibatch no k-means
                                random_state=0)

t0 = time()

lda.fit(vec_text)
doc_topic_matrix = lda.transform(vec_text)

print("done in %0.3fs." % (time() - t0))

print('Matriz documento-tópicos:' + str(doc_topic_matrix.shape))
print('Matriz tópicos-termos:' + str(lda.components_.shape))

dis = display_topics(doc_topic_matrix,
               lda.components_, 
               words,
               df,
               15, 
               10)

print(dis)
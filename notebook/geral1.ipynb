{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/renato/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2c8369ac448c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageColorGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from imageio import imread\n",
    "import warnings\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "\n",
    "import altair as alt\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refinando stop word\n",
    "stop_words = set(stopwords.words(\"portuguese\"))\n",
    "stop_words.update(['que', 'até', 'esse', \n",
    "                    'essa', 'pro', 'pra',\n",
    "                    'oi', 'lá', 'blá', 'dos', 'esta'])\n",
    "const_words = [ 'de','a','o','que','e','do','da','em','um','para','e','com','nao','uma','os','no','vou','se','na','por','mais','as','dos','como','mas','foi','ao','ele','das','tem','a','seu','sua','ou','ser','quando','muito','ha','nos','ja','esta','eu','tambem','so','pelo','pela','ate','isso','ela','entre','era','depois','sem','mesmo','aos','ter','seus','quem','nas','me','esse','eles','estao','voce','tinha','foram','essa','num','nem','suas','meu','as','minha','tem','numa','pelos','elas','havia','seja','qual','sera','nos','tenho','lhe','deles','essas','esses','pelas','este','fosse','dele','tu','te','voces','vos','lhes','meus','minhas','teu','tua','teus','tuas','nosso','nossa','nossos','nossas','dela','delas','esta','estes','estas','aquele','aquela','aqueles','aquelas','isto','aquilo','estou','esta','estamos','estao','estive','esteve','estivemos','estiveram','estava','estavamos','estavam','estivera','estiveramos','esteja','estejamos','estejam','estivesse','estivessemos','estivessem','estiver','estivermos','estiverem','hei','ha','havemos','hao','houve','houvemos','houveram','houvera','houveramos','haja','hajamos','hajam','houvesse','houvessemos','houvessem','houver','houvermos','houverem','houverei','houvera','houveremos','houverao','houveria','houveriamos','houveriam','sou','somos','sao','era','eramos','eram','fui','foi','fomos','foram','fora','foramos','seja','sejamos','sejam','fosse','fossemos','fossem','for','formos','forem','serei','sera','seremos','serao','seria','seriamos','seriam','tenho','tem','temos','tem','tinha','tinhamos','tinham','tive','teve','tivemos','tiveram','tivera','tiveramos','tenha','tenhamos','tenham','tivesse','tivessemos','tivessem','tiver','tivermos','tiverem','terei','tera','teremos','terao','teria','teriamos','teriam','a', 'à', 'adeus', 'agora', 'aí', 'ainda', 'além', 'algo', 'alguém', 'algum', 'alguma', 'algumas', 'alguns', 'ali', 'ampla', 'amplas', 'amplo', 'amplos', 'ano', 'anos', 'ante', 'antes', 'ao', 'aos', 'apenas', 'apoio', 'após', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aqui', 'aquilo', 'área', 'as', 'às', 'assim', 'até', 'atrás', 'através', 'baixo', 'bastante', 'bem', 'boa', 'boas', 'bom', 'bons', 'breve', 'cá', 'cada', 'catorze', 'cedo', 'cento', 'certamente', 'certeza', 'cima', 'cinco', 'coisa', 'coisas', 'com', 'como', 'conselho', 'contra', 'contudo', 'custa', 'da', 'dá', 'dão', 'daquela', 'daquelas', 'daquele', 'daqueles', 'dar', 'das', 'de', 'debaixo', 'dela', 'delas', 'dele', 'deles', 'demais', 'dentro', 'depois', 'desde', 'dessa', 'dessas', 'desse', 'desses', 'desta', 'destas', 'deste', 'destes', 'deve', 'devem', 'devendo', 'dever', 'deverá', 'deverão', 'deveria', 'deveriam', 'devia', 'deviam', 'dez', 'dezanove', 'dezasseis', 'dezassete', 'dezoito', 'dia', 'diante', 'disse', 'disso', 'disto', 'dito', 'diz', 'dizem', 'dizer', 'do', 'dois', 'dos', 'doze', 'duas', 'dúvida', 'e', 'é', 'ela', 'elas', 'ele', 'eles', 'em', 'embora', 'enquanto', 'entre', 'era', 'eram', 'éramos', 'és', 'essa', 'essas', 'esse', 'esses', 'esta', 'está', 'estamos', 'estão', 'estar', 'estas', 'estás', 'estava', 'estavam', 'estávamos', 'este', 'esteja', 'estejam', 'estejamos', 'estes', 'esteve', 'estive', 'estivemos', 'estiver', 'estivera', 'estiveram', 'estivéramos', 'estiverem', 'estivermos', 'estivesse', 'estivessem', 'estivéssemos', 'estiveste', 'estivestes', 'estou', 'etc', 'eu', 'exemplo', 'faço', 'falta', 'favor', 'faz', 'fazeis', 'fazem', 'fazemos', 'fazendo', 'fazer', 'fazes', 'feita', 'feitas', 'feito', 'feitos', 'fez', 'fim', 'final', 'foi', 'fomos', 'for', 'fora', 'foram', 'fôramos', 'forem', 'forma', 'formos', 'fosse', 'fossem', 'fôssemos', 'foste', 'fostes', 'fui', 'geral', 'grande', 'grandes', 'grupo', 'há', 'haja', 'hajam', 'hajamos', 'hão', 'havemos', 'havia', 'hei', 'hoje', 'hora', 'horas', 'houve', 'houvemos', 'houver', 'houvera', 'houverá', 'houveram', 'houvéramos', 'houverão', 'houverei', 'houverem', 'houveremos', 'houveria', 'houveriam', 'houveríamos', 'houvermos', 'houvesse', 'houvessem', 'houvéssemos', 'isso', 'isto', 'já', 'la', 'lá', 'lado', 'lhe', 'lhes', 'lo', 'local', 'logo', 'longe', 'lugar', 'maior', 'maioria', 'mais', 'mal', 'mas', 'máximo', 'me', 'meio', 'menor', 'menos', 'mês', 'meses', 'mesma', 'mesmas', 'mesmo', 'mesmos', 'meu', 'meus', 'mil', 'minha', 'minhas', 'momento', 'muita', 'muitas', 'muito', 'muitos', 'na', 'nada', 'não', 'naquela', 'naquelas', 'naquele', 'naqueles', 'nas', 'nem', 'nenhum', 'nenhuma', 'nessa', 'nessas', 'nesse', 'nesses', 'nesta', 'nestas', 'neste', 'nestes', 'ninguém', 'nível', 'no', 'noite', 'nome', 'nos', 'nós', 'nossa', 'nossas', 'nosso', 'nossos', 'nova', 'novas', 'nove', 'novo', 'novos', 'num', 'numa', 'número', 'nunca', 'o', 'obra', 'obrigada', 'obrigado', 'oitava', 'oitavo', 'oito', 'onde', 'ontem', 'onze', 'os', 'ou', 'outra', 'outras', 'outro', 'outros', 'para', 'parece', 'parte', 'partir', 'paucas', 'pela', 'pelas', 'pelo', 'pelos', 'pequena', 'pequenas', 'pequeno', 'pequenos', 'per', 'perante', 'perto', 'pode', 'pude', 'pôde', 'podem', 'podendo', 'poder', 'poderia', 'poderiam', 'podia', 'podiam', 'põe', 'põem', 'pois', 'ponto', 'pontos', 'por', 'porém', 'porque', 'porquê', 'posição', 'possível', 'possivelmente', 'posso', 'pouca', 'poucas', 'pouco', 'poucos', 'primeira', 'primeiras', 'primeiro', 'primeiros', 'própria', 'próprias', 'próprio', 'próprios', 'próxima', 'próximas', 'próximo', 'próximos', 'pude', 'puderam', 'quais', 'quáis', 'qual', 'quando', 'quanto', 'quantos', 'quarta', 'quarto', 'quatro', 'que', 'quê', 'quem', 'quer', 'quereis', 'querem', 'queremas', 'queres', 'quero', 'questão', 'quinta', 'quinto', 'quinze', 'relação', 'sabe', 'sabem', 'são', 'se', 'segunda', 'segundo', 'sei', 'seis', 'seja', 'sejam', 'sejamos', 'sem', 'sempre', 'sendo', 'ser', 'será', 'serão', 'serei', 'seremos', 'seria', 'seriam', 'seríamos', 'sete', 'sétima', 'sétimo', 'seu', 'seus', 'sexta', 'sexto', 'si', 'sido', 'sim', 'sistema', 'só', 'sob', 'sobre', 'sois', 'somos', 'sou', 'sua', 'suas', 'tal', 'talvez', 'também', 'tampouco', 'tanta', 'tantas', 'tanto', 'tão', 'tarde', 'te', 'tem', 'tém', 'têm', 'temos', 'tendes', 'tendo', 'tenha', 'tenham', 'tenhamos', 'tenho', 'tens', 'ter', 'terá', 'terão', 'terceira', 'terceiro', 'terei', 'teremos', 'teria', 'teriam', 'teríamos', 'teu', 'teus', 'teve', 'ti', 'tido', 'tinha', 'tinham', 'tínhamos', 'tive', 'tivemos', 'tiver', 'tivera', 'tiveram', 'tivéramos', 'tiverem', 'tivermos', 'tivesse', 'tivessem', 'tivéssemos', 'tiveste', 'tivestes', 'toda', 'todas', 'todavia', 'todo', 'todos', 'trabalho', 'três', 'treze', 'tu', 'tua', 'tuas', 'tudo', 'última', 'últimas', 'último', 'últimos', 'um', 'uma', 'umas', 'uns', 'vai', 'vais', 'vão', 'vários', 'vem', 'vêm', 'vendo', 'vens', 'ver', 'vez', 'vezes', 'viagem', 'vindo', 'vinte', 'vir', 'você', 'vocês', 'vos', 'vós', 'vossa', 'vossas', 'vosso', 'vossos', 'zero', 'the', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', '_' ]\n",
    "wrd = [w for w in const_words if w not in stop_words]\n",
    "# contem 593 stop words em portugues\n",
    "stop_words.update(wrd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conecta com banco de dados\n",
    "import pymysql\n",
    "def get_conexao():\n",
    "    return  pymysql.connect(\n",
    "        user=\"renato\",\n",
    "        host=\"200.131.5.71\",\n",
    "        database=\"Monitor_de_noticia\",\n",
    "        password='renato20',\n",
    "    )\n",
    "\n",
    "\n",
    "def get_data_base(tabela, con):\n",
    "    return pd.read_sql(f'select * from {tabela};', con=con) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importação dos dados\n",
    "con = get_conexao()\n",
    "tabela = 'tweets_aleatorio'\n",
    "df = get_data_base(tabela, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_nome</th>\n",
       "      <th>tweet</th>\n",
       "      <th>user_fk</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shoppingchina</td>\n",
       "      <td>! De esta manera #ShoppingChina agradece a tod...</td>\n",
       "      <td>554</td>\n",
       "      <td>2020-12-24 12:43:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AmandaG36643728</td>\n",
       "      <td>! https://t.co/2AmKKASsdI</td>\n",
       "      <td>407</td>\n",
       "      <td>2020-12-23 14:55:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Almeida_755</td>\n",
       "      <td>! https://t.co/5iTY6EA0Tv</td>\n",
       "      <td>977</td>\n",
       "      <td>2020-12-21 13:45:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>celoduardo</td>\n",
       "      <td>! https://t.co/7qE0Zyn7El</td>\n",
       "      <td>611</td>\n",
       "      <td>2020-11-15 20:28:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anaritanovaess</td>\n",
       "      <td>! https://t.co/9cC1Os5VN5</td>\n",
       "      <td>1072</td>\n",
       "      <td>2020-12-23 18:57:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146258</th>\n",
       "      <td>luissparis</td>\n",
       "      <td>⬇ Ferreirinha \\n⬆ Piñares</td>\n",
       "      <td>226</td>\n",
       "      <td>2020-12-04 02:02:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146259</th>\n",
       "      <td>jujucec</td>\n",
       "      <td>⭐ https://t.co/gpa3MweRts</td>\n",
       "      <td>537</td>\n",
       "      <td>2020-12-21 22:39:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146260</th>\n",
       "      <td>henrySouzah1</td>\n",
       "      <td>ツAs  Vezes Oc Perde O Que Não \\nQueria, Mais C...</td>\n",
       "      <td>113</td>\n",
       "      <td>2019-12-13 13:37:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146261</th>\n",
       "      <td>Kaykydocpxo</td>\n",
       "      <td>你為什麼翻譯？ 想和我約會？</td>\n",
       "      <td>316</td>\n",
       "      <td>2020-12-24 03:39:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146262</th>\n",
       "      <td>Fernand92402347</td>\n",
       "      <td>変身してほしくない特撮ヒロイン https://t.co/U492FFUea3 via @Y...</td>\n",
       "      <td>545</td>\n",
       "      <td>2020-12-14 23:59:31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>146263 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              user_nome                                              tweet  \\\n",
       "0         shoppingchina  ! De esta manera #ShoppingChina agradece a tod...   \n",
       "1       AmandaG36643728                          ! https://t.co/2AmKKASsdI   \n",
       "2           Almeida_755                          ! https://t.co/5iTY6EA0Tv   \n",
       "3            celoduardo                          ! https://t.co/7qE0Zyn7El   \n",
       "4        anaritanovaess                          ! https://t.co/9cC1Os5VN5   \n",
       "...                 ...                                                ...   \n",
       "146258       luissparis                          ⬇ Ferreirinha \\n⬆ Piñares   \n",
       "146259          jujucec                          ⭐ https://t.co/gpa3MweRts   \n",
       "146260     henrySouzah1  ツAs  Vezes Oc Perde O Que Não \\nQueria, Mais C...   \n",
       "146261      Kaykydocpxo                                     你為什麼翻譯？ 想和我約會？   \n",
       "146262  Fernand92402347  変身してほしくない特撮ヒロイン https://t.co/U492FFUea3 via @Y...   \n",
       "\n",
       "        user_fk                 data  \n",
       "0           554  2020-12-24 12:43:57  \n",
       "1           407  2020-12-23 14:55:35  \n",
       "2           977  2020-12-21 13:45:49  \n",
       "3           611  2020-11-15 20:28:56  \n",
       "4          1072  2020-12-23 18:57:52  \n",
       "...         ...                  ...  \n",
       "146258      226  2020-12-04 02:02:39  \n",
       "146259      537  2020-12-21 22:39:58  \n",
       "146260      113  2019-12-13 13:37:37  \n",
       "146261      316  2020-12-24 03:39:03  \n",
       "146262      545  2020-12-14 23:59:31  \n",
       "\n",
       "[146263 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters and digits\n",
    "def clean_noticia(noticia):\n",
    "   \n",
    "    # remove links e alguns pontos\n",
    "    noticia = re.sub(r'<img src=\"https\\S+', \"\", noticia)\n",
    "    noticia = re.sub(r'href\\S+', \"\", noticia)\n",
    "    noticia = re.sub(r'rel', \"\", noticia)\n",
    "    noticia = re.sub(r'nofollow\\S+', \"\", noticia)\n",
    "    noticia = re.sub(r'mixvale\\S+', \"\", noticia)\n",
    "    noticia = re.sub(r'target\\S+', \"\", noticia)\n",
    "    noticia = re.sub(r'_blank\\S+', \"\", noticia)\n",
    "    noticia = re.sub(r'www\\S+', \"\", noticia)\n",
    "    noticia = re.sub(r\"http\\S+\", \"\", noticia).lower().replace('.','').replace(';','').replace('-','').replace(':','').replace(')','')\n",
    "    \n",
    "    # remove alguns caracteres\n",
    "    noticia  = re.sub(\"(\\\\d|\\\\W)+|\\w*\\d\\w*\",\" \",noticia )\n",
    "    noticia = ' '.join(s for s in noticia.split() if (not any(c.isdigit() for c in s)) and len(s) > 2)\n",
    "    noticia = noticia.replace(\"\\n\", \"\")\n",
    "    \n",
    "    #remove stopwords\n",
    "    noticia = ' '.join([w for w in noticia.split() if w not in stop_words])\n",
    "    return noticia\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    # remove os RT\n",
    "    tweet = re.sub(r'RT+', '', tweet) \n",
    "    \n",
    "    # remove as menções\n",
    "    tweet = re.sub(r'@\\S+', '', tweet)  \n",
    "    \n",
    "    # remove links e alguns pontos\n",
    "    tweet = re.sub(r'kkk\\S+', '', tweet)\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet).lower().replace('.','').replace(';','').replace('-','').replace(':','').replace(')','')\n",
    "    \n",
    "    # remove alguns caracteres\n",
    "    tweet  = re.sub(\"(\\\\d|\\\\W)+|\\w*\\d\\w*\",\" \",tweet )\n",
    "    tweet = ' '.join(s for s in tweet.split() if (not any(c.isdigit() for c in s)) and len(s) > 2)\n",
    "    tweet = tweet.replace(\"\\n\", \"\")\n",
    "    tweet = ' '.join([w for w in tweet.split() if w not in stop_words])\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noticia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chopp acho gosto aniversário pais',\n",
       " 'bieber nega edição imagens campanha vira meme',\n",
       " 'pai amy winehouse filha acompanha palco ambrosio',\n",
       " 'zagueiro alemanha quis humilhar brasil ambrosio',\n",
       " 'amizade amor morre mário quintana',\n",
       " 'bolsa valores diferente economia produção bens serviços existem fanta',\n",
       " 'data entrega amanhã vtnc classroom',\n",
       " 'mãe',\n",
       " 'mamãe',\n",
       " 'pressa vacina justifica mexe vida pessoas inglaterra come',\n",
       " 'aaah fácil mora sozinha aonde vida fácil bota cara frente',\n",
       " 'abre porta kkkkkkkk',\n",
       " 'acharam queiroz gente acha clítoris cantadas progressistas',\n",
       " 'acredite capacidades única pessoa capaz mudar rumo vida quartadetremurasdv',\n",
       " 'liso fodaci empresa vende jogo console interessa porra respons',\n",
       " 'inovações inova casa more',\n",
       " 'aprende prática aprendo passar raiva raloooooos conhecimentos sso gal',\n",
       " 'passar pano djonga pano',\n",
       " 'gays abrem relacionamento promiscuidade flor marido traíram',\n",
       " 'precisar gente precisa pessoa some',\n",
       " 'monte contatinho mano soubesse vácuos firmes calava porra boca',\n",
       " 'ain racista pai preto cabelo duro ããã pessoa fala imagino',\n",
       " 'ain generais frouxo ain precisa cabo soldado amigo delegue respons',\n",
       " 'ain relativístico mecânica clássica meme errado blablablabla mama mit mlk chato',\n",
       " 'ain senador faça força darem andamento pedido impeachment ministros stf amig',\n",
       " 'ain remo paysandu jogo cumpadre subirem foder remo',\n",
       " 'ain senadores revoltados acredita nisso',\n",
       " 'senhor converteivos mim coração jejuns choro',\n",
       " 'alcanza con día recuerda joker una persona cualquiera pierda cabeza descienda hacia',\n",
       " 'prevenia filho mundo sentia necessidade vida buscava amor',\n",
       " 'ache dinheiro saco trecho procuro fica ecoando cabeça',\n",
       " 'ambientalismo pensa luta classes jardinagem',\n",
       " 'amigo contigo ovo bife',\n",
       " 'and couldn stand person inside turned all mirrors around',\n",
       " 'redor buraco beira sextando ariano suassuna']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tweets = []\n",
    "for w in range(len(df.tweet)):\n",
    "  tweet = df['tweet'].iloc[w]\n",
    "  tweet = clean_tweet(tweet)\n",
    "    \n",
    "  clean_tweets.append(tweet)\n",
    "\n",
    "\n",
    "clean_tweets[110:145]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = np.array(clean_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelagem de Tópicos LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3486\n"
     ]
    }
   ],
   "source": [
    "# COUNT vectorizer\n",
    "tf_vectorizer = CountVectorizer(\n",
    "        min_df = 30,\n",
    "        max_df = 0.5,\n",
    "        max_features = 10000,\n",
    "        stop_words = stop_words, \n",
    "        ngram_range = (1,2)\n",
    "  )\n",
    "\n",
    "#transform\n",
    "vec_text = tf_vectorizer.fit_transform(clean_tweets)\n",
    "\n",
    "#returns a list of words.\n",
    "words = tf_vectorizer.get_feature_names()\n",
    "#print(clean_tweets)\n",
    "\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "  for topic_idx, topic in enumerate(model.components_):\n",
    "    print(\"\\n--\\nTopic #{}: \".format(topic_idx + 1))\n",
    "    message = \", \".join([feature_names[i]\n",
    "                          for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    print(message)\n",
    "  print()\n",
    "\n",
    "def display_topics(W, H, feature_names, documents, no_top_words, no_top_documents):\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        print(\"\\n--\\nTopic #{}: \".format(topic_idx + 1))\n",
    "        print(\", \".join([feature_names[i]\n",
    "                for i in topic.argsort()[:-no_top_words - 1:-1]]).upper())\n",
    "        top_d_idx = np.argsort(W[:,topic_idx])[::-1][0:no_top_documents]\n",
    "        for d in top_d_idx: \n",
    "          doc_data = df[['TITLE', 'SUMMARY']].iloc[d]\n",
    "          print(f'{doc_data[0]} -  \\t{W[d, topic_idx]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=15, \n",
    "                                learning_method='online', # 'online' equivale a minibatch no k-means\n",
    "                                random_state=0)\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "lda.fit(vec_text)\n",
    "doc_topic_matrix = lda.transform(vec_text)\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequencia de palavras + WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word cloud\n",
    "\n",
    "words = ' '.join(df['text'])\n",
    "warnings.simplefilter('ignore')\n",
    "#twitter_mask = imread('brasil_mask.png')\n",
    "\n",
    "wc = WordCloud(min_font_size=10,\n",
    "              max_font_size=300,\n",
    "              background_color='white',\n",
    "              mode=\"RGB\",\n",
    "              width=2000,\n",
    "              height=1000,\n",
    "              #mask=twitter_mask,\n",
    "              normalize_plurals = True).generate(words)\n",
    "\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "fig = plt.gcf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analise de Sentimento"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}